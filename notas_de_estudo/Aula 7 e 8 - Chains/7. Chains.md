## Defini√ß√£o

Se voc√™ acompanhou as aulas anteriores, voc√™ consegue visualizar agora que todos os componentes LangChain apresentados, podem ser encadeados, uma vez que eles implementam a interface `Runnable`, ou seja, podemos montar nossas cadeias (chains) e, vale lembrar que a sa√≠da de um componente ser√° a entrada do pr√≥ximo componente.

Chain portanto √© a cadeia formado por elementos de LangChain que implementam uma determinada a√ß√£o, principalmente envolvendo a atua√ß√£o de LLMs. Ou seja, a chain nada mais √© do que amarrar uma s√©rie de tarefas realizadas por cada componente em um √∫nico fluxo linear.

Para esta aula preparei algo mais pr√°tico, ou seja, vamos praticar diferentes formas de encadeamento de componentes, por isso √© fundamental voc√™ ter visto as aulas anteriores e entender tamb√©m os `runnables` j√° que vamos utiliz√°-los em nossos exerc√≠cios.

## Exemplo 1 - B√°sico

Vamos criar uma chain que √© a mais simples envolvendo um prompt, um modelo e um analisador de sa√≠da. J√° vimos em aulas anteriores, mas vale relembrar e √© um exemplo bom para come√ßarmos at√© aprofundar nas cadeias mais complexas.

```python
from dotenv import load_dotenv  
from langchain.prompts import ChatPromptTemplate  
from langchain.schema.output_parser import StrOutputParser  
from langchain_openai import ChatOpenAI  
  
# Carregar as chas APIs presentes no arquivo .env  
load_dotenv()  
  
# Instancias um chatmodel para comunicarmos com os modelos LLMs  
model = ChatOpenAI(model="gpt-4o", temperature=0.5)  
  
# Definindo o prompt de comunica√ß√£o - adotamos aqui um estilo chat prompt  
# template, uma vez que estamos usando o modelo do tipo chat.  
  
prompt_sistema = """Voc√™ √© um assistente especialista em criar conte√∫do para o twitter e tem como objetivo \  
criar os melhores tweets virais sobre o tema que o usu√°rio te passar. Seja criativo e atenda ao padr√£o de 280 caracteres do \  
twitter.  
"""  
  
prompt_template = ChatPromptTemplate(  
    [  
        ("system", prompt_sistema),  
        ("human", "Crie um total de {numero_de_publicacoes} tweets sobre o tema {input_tema}."),  
    ]  
)  
  
# Crie a cadeia combinada usando LangChain Expression Language (LCEL)  
chain = prompt_template | model | StrOutputParser()  
  
  
# Executamos nossa chain  
result = chain.invoke({"numero_de_publicacoes": 3, "input_tema": "tecnologia"})  
  
# Imprimimos a sa√≠da.  
print(result)
```

## Exemplo 2 - Chains sequenciais com fun√ß√µes personalizadas

Vamos criar uma chain agora envolvendo a cria√ß√£o de fun√ß√µes personalizadas, ou seja, acoplaremos aqui a atua√ß√£o dos `RunnableLambda` para que possamos converter uma fun√ß√£o personalizada em componente LangChain e ser possivel adicionar nossa fun√ß√£o em meio √† cadeia.

Digamos que n√≥s queremos confirmar se o total de caracteres de cada tweet est√° dentro dos 280 caracteres permitido. Vamos aproveitar o nosso exemplo anterior sobre a cria√ß√£o de tweets para gerar um relat√≥rio de valida√ß√£o.

1. Vamos primeiro ajustar nosso prompt para que o LLM sempre gere o total de tweets separados por uma quebra de linha (\n).
2. Em seguida, vamos construir uma fun√ß√£o que funcionar√° como um analisador de saida personalizado ou seja, pegar√° a sa√≠da `string` do modelo e ir√° gerar uma lista em que cada elemento dessa lista ser√° um tweet criado pelo LLM. Aqui usaremos o `RunnableLambda` para nos ajudar.
3. Depois teremos uma outra fun√ß√£o personalizada que analisa essa lista e conta o total de caracteres de cada tweet. O retorno dela ser√° um dicion√°rio com os resultado. Aqui usaremos o `RunnableLambda` para nos ajudar tamb√©m.

```python
from dotenv import load_dotenv  
from langchain.prompts import ChatPromptTemplate  
from langchain.schema.output_parser import StrOutputParser  
from langchain_openai import ChatOpenAI  
from langchain_core.runnables import RunnableLambda  
  
# Carregar as chas APIs presentes no arquivo .env  
load_dotenv()  
  
# ------------------------------------------------------------------------------
  
# Instancias um chatmodel para comunicarmos com os modelos LLMs  
model = ChatOpenAI(model="gpt-4o", temperature=0.5)  
  
# ------------------------------------------------------------------------------
# Criando uma fun√ß√£o personalizada para tratar a saida textual do LLM  
  
def separador_de_tweet(entrada: str) -> list:  
    """  
    Fun√ß√£o que recebe uma string e retorna uma lista com os elementos separados por quebras de linha.  
    Args:        entrada (str): A string de entrada, onde os valores est√£o separados por quebras de linha.  
    Returns:        list: Uma lista contendo cada elemento da string como um item separado.    """    # Divide a string em uma lista utilizando o caractere de quebra de linha '\n'  
    elementos = entrada.split('\n')  
  
    # Remove espa√ßos extras e ignora linhas vazias  
    elementos_limpos = [elemento.strip() for elemento in elementos if elemento.strip()]  
  
    return elementos_limpos  
  
# Criando uma fun√ß√£o personalizada pegar a lista criada na fun√ß√£o anterior e gerar um dicion√°rio com o rel√°torio de  
# analise de caracteres.  
  
def relatorio_de_analise_de_caracteres(entrada: list) -> dict:  
    """  
    Fun√ß√£o que gera um relat√≥rio com os tweets e a contagem de caracteres de cada tweet.  
    Args:        entrada (list): Lista de strings representando os tweets.  
    Returns:        dict: Um dicion√°rio com duas chaves:              - 'tweets': contendo a lista original.              - 'num_caract': contendo uma lista com o n√∫mero de caracteres de cada tweet.    """    # Gera a contagem de caracteres para cada item na lista  
    contagem_caracteres = [len(tweet) for tweet in entrada]  
  
    # Monta o dicion√°rio de sa√≠da  
    relatorio = {  
        'tweets': entrada,  
        'num_caract': contagem_caracteres  
    }  
  
    return relatorio  
  
# ------------------------------------------------------------------------------
  
# Definindo o prompt de comunica√ß√£o - adotamos aqui um estilo chat prompt  
# template, uma vez que estamos usando o modelo do tipo chat.  
  
prompt_sistema = """Voc√™ √© um assistente especialista em criar conte√∫do para o twitter e tem como objetivo \  
criar os melhores tweets virais sobre o tema que o usu√°rio te passar. Seja criativo e atenda ao padr√£o de 280 caracteres do \  
twitter.  
Orienta√ß√£o:  
- Crie apenas o numero de tweets informado.  
- Separe cada um deles por uma quebra de linha,  
"""  
  
prompt_template = ChatPromptTemplate(  
    [  
        ("system", prompt_sistema),  
        ("human", "Crie um total de {numero_de_publicacoes} tweets sobre o tema {input_tema}."),  
    ])  
  
# ------------------------------------------------------------------------------ 
  
  
# Crie a cadeia combinada usando LangChain Expression Language (LCEL).  
# para adicionar os outros componentes personalizados √† cadeia, precisamos converter as fun√ß√µes em um componente langchain, para isso  
# precisamos usar o RunnableLambda.  
  
chain = prompt_template | model | StrOutputParser() | RunnableLambda(separador_de_tweet) | RunnableLambda(relatorio_de_analise_de_caracteres)  
  
# ------------------------------------------------------------------------------
  
# Executamos nossa chain  
result = chain.invoke({"numero_de_publicacoes": 3, "input_tema": "tecnologia"})  
  
# Imprimimos o nosso dicion√°rio de relat√≥rio:  
print(result)  
print("-"*50)  
  
# imprimindo de forma mais estruturada:  
for i, (tweet, num_caract) in enumerate(zip(result['tweets'], result['num_caract']), start=1):  
        print(f"Tweet {i}: {tweet}")  
        print(f"Total de caracteres: {num_caract}")  
        if num_caract <= 280:  
            print("Valida√ß√£o: OK")  
        else:  
            print("Valida√ß√£o: Tweet supera o limite de 280 caracteres")  
        print("-"*50)
```

## Exemplo 3 - Chains de execu√ß√£o paralela

Agora vamos a um problema mais complexo, imagine que queremos criar um assistente que recebe uma review de um filme e ele tem que analisar pr√≥s e contras comentados no review entregue na entrada.

Para que possamos entender a execu√ß√£o paralela de chain, vamos criar uma arquitetura onde a an√°lise pr√≥ e contra s√£o realizadas de forma paralela, e no final vamos unir os resultados.

```python
from dotenv import load_dotenv  
from langchain.prompts import ChatPromptTemplate  
from langchain.schema.output_parser import StrOutputParser  
from langchain.schema.runnable import RunnableLambda  
from langchain_openai import ChatOpenAI  
  
# Carregar as chas APIs presentes no arquivo .env  
load_dotenv()  
  
# -------------------------------------------------------------------------------- 
  
# Instancias um chatmodel para comunicarmos com os modelos LLMs  
model = ChatOpenAI(model="gpt-4o", temperature=0.2)  
  
# Definindo o prompt de comunica√ß√£o - adotamos aqui um estilo chat prompt  
# template, uma vez que estamos usando o modelo do tipo chat.  
  
prompt_template = ChatPromptTemplate.from_messages(  
    [  
        ("system", "Voc√™ √© um escritor especialista em an√°lises de review de filmes de cinema."),  
        ("human", "Liste de forma estruturada os principais detalhes e pontos de vistas apresentados na seguinte \  
review entregue pelo usu√°rio n√£o invente nada apenas capture as principais informa√ß√µes apresentadas.\  
Review: {movie_review}."),  
    ]  
)  
  
# -------------------------------------------------------------------------------- 
  
# Vamos definir um bra√ßo da nossa chain que ser√° uma cadeia intermedi√°ria de analise dos pontos positivos sobre a review.  
  
analise_ponto_positivo_template = ChatPromptTemplate(  
    [  
        ("system", "Voc√™ √© um analista cr√≠tico de filmes de cinema"),  
        (  
            "human", "Dados este review estruturado: {review_estruturado}, liste os pontos positivos do filme.",  
        ),  
    ]  
)  
  
  
# criando a chain do bra√ßo 1  
chain_intermediaria_positiva = analise_ponto_positivo_template | model | StrOutputParser()  
  
# -------------------------------------------------------------------------------- 
  
# Vamos definir outro bra√ßo da nossa chain que ser√° uma cadeia intermedi√°ria de analise dos pontos negativos sobre a review.  
  
analise_ponto_negativo_template = ChatPromptTemplate(  
    [  
        ("system", "Voc√™ √© um analista cr√≠tico de filmes de cinema"),  
        (  
            "human", "Dados este review estruturado: {review_estruturado}, liste os pontos negativos do filme.",  
        ),  
    ]  
)  
  
# criando a chain do bra√ßo 2  
chain_intermediaria_negativa = analise_ponto_negativo_template | model | StrOutputParser()  
  
# -------------------------------------------------------------------------------- 
  
# Fun√ß√£o respons√°vel por combinar os resultados dos bra√ßos que v√£o ser executados em paralelo.  
def combinando_analises(entrada: dict):  
    return f"An√°lise positiva:\n{entrada['posivita']}\n\nAn√°lise negativa:\n{entrada['negativa']}"  
# -------------------------------------------------------------------------------- 
  
# Crie a cadeia combinada usando LangChain Expression Language (LCEL)  
# Em RunnableLambda(lambda x: {"review_estruturado": x}) estamos convertendo a saida string para um dicion√°rio  
# com a chave 'review_estruturado' que os templates das chains intermedi√°rias exige como entrada.  
  
chain = (prompt_template  
         | model  
         | StrOutputParser()  
         | RunnableLambda(lambda x: {"review_estruturado": x})  
         | {"posivita": chain_intermediaria_positiva, "negativa": chain_intermediaria_negativa}  
         | RunnableLambda(combinando_analises)  
         )  
  
# -------------------------------------------------------------------------------- 
  
# Executando nossa chain principal.  
movie_review ="""Cr√≠tica de "O Gladiador 2"  
"O Gladiador 2", dirigido por Ridley Scott, chega aos cinemas com a expectativa de reviver a grandiosidade √©pica de \  
seu antecessor. No entanto, apesar do hist√≥rico impressionante de Scott, que inclui cl√°ssicos como "Blade Runner" e \  
"Alien", o filme parece trope√ßar em sua pr√≥pria ambi√ß√£o.  
  
Desde o in√≠cio, o filme tenta inovar ao incorporar cenas animadas e elementos de intelig√™ncia artificial, \  
possivelmente como uma homenagem ao primeiro "Gladiador". No entanto, essa escolha est√©tica, embora ousada, \  
n√£o se integra de maneira fluida √† narrativa, criando uma desconex√£o que pode confundir o espectador.  
  
A tentativa de trazer novidade √†s lutas no Coliseu, com a introdu√ß√£o de navios vikings e tubar√µes, √© um exemplo \  
de como o filme busca surpreender. No entanto, essas cenas acabam por sacrificar a autenticidade hist√≥rica em prol \  
do espet√°culo, o que pode afastar aqueles que esperavam uma representa√ß√£o mais fiel das arenas romanas. A inclus√£o \  
de macacos em combate, por sua vez, remete a outras franquias cinematogr√°ficas, diluindo ainda mais a originalidade \  
do enredo.  
  
Apesar dessas escolhas question√°veis, √© importante reconhecer o esfor√ßo de Scott em tentar oferecer algo novo e \  
visualmente impactante. No entanto, a falta de uma pesquisa hist√≥rica mais aprofundada se faz sentir, e o filme \  
poderia ter se beneficiado de uma abordagem mais cuidadosa nesse aspecto.  
  
Em suma, "O Gladiador 2" √© uma obra que, embora repleta de potencial e com momentos de brilho visual, acaba por se \  
perder em sua tentativa de inovar. Para os f√£s do g√™nero e do diretor, pode ser uma experi√™ncia mista, que levanta \  
quest√µes sobre at√© que ponto a inova√ß√£o deve ir sem comprometer a ess√™ncia e a coer√™ncia da narrativa.  
"""  
  
result = chain.invoke({"movie_review": movie_review})  
  
# --------------------------------------------------------------------------------
# Imprimindo a saida.  
print(result)
```

## Exemplo 4 - Chain de roteamento (Branch)

Agora vamos a um problema onde teremos uma fun√ß√£o de roteamento. Essa fun√ß√£o ter√° a fun√ß√£o de decidir, usando a classifica√ß√£o por meio de um LLM, para qual branch (bra√ßo/ramo) da nossa chain o programa deve seguir e finalizar.

Imagine um sistema de atendimento com duas rotas: (1) quando o usu√°rio solicita que seja atendido por um  humano, encaminhamos a pergunta para um atendente humano e finalizamos a cadeia ou (2) quando o usu√°rio apenas deseja tirar d√∫vidas sobre um produto, nosso sistema escolhe enviar a pergunta para uma chain que implementa um bot capaz de responder √†s d√∫vidas desse usu√°rio e finaliza a itera√ß√£o.

A pe√ßa principal desse nosso sistema est√° na cria√ß√£o de uma fun√ß√£o que far√° esse roteamento entre cadeias de LangChain. No nosso caso, voc√™ pode observar isso na fun√ß√£o 'executa_roteamento' implementada:

```python
from dotenv import load_dotenv  
from langchain.prompts import ChatPromptTemplate  
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser  
from langchain.schema.runnable import RunnableLambda  
from langchain_openai import ChatOpenAI  
  
from pydantic import BaseModel, Field 
# Carregar as chas APIs presentes no arquivo .env  
load_dotenv()  
  
# -------------------------------------------------------------------------------- 
  
# Instancias um chatmodel para comunicarmos com os modelos LLMs  
model = ChatOpenAI(model="gpt-4o", temperature=0.2)  
  
# -------------------------------------------------------------------------------- 
## Definindo a estrutura da chain que vai avaliar a entrada e retornar uma classifica√ß√£o para nossa fun√ß√£o 'executa_roteamento'  
# Definindo a minha estrutura de sa√≠da usando Pydantic  
class Rota(BaseModel):  
    opcao: bool = Field(description="Defina True se necessitar atendimento humano e false caso contr√°rio.")  
    pergunta_user: str = Field(description="Colocar neste parametro a pergunta do usu√°rio sem alter√°-la.")  
  
  
parser = PydanticOutputParser(pydantic_object=Rota)  
  
sys_prompt_rota = """Voc√™ √© um especialista em classifica√ß√£o. Voc√™ receber√° perguntas do usu√°rio e precisar√° classificar, \  
de forma booleana, se o usu√°rio est√° solicitando conversar com um atendente humano ou n√£o.  
\n{format_instructions}\n  
Pergunta Usu√°rio: {pergunta_user}"  
"""  
  
rota_prompt_template = ChatPromptTemplate([("system", sys_prompt_rota),],  
                                          partial_variables={"format_instructions": parser.get_format_instructions()}  
                                          )  
  
# criando o peda√ßo da chain que controla o roteamento entre as branches  
chain_de_roteamento = rota_prompt_template | model | parser  
  
# Se quiser testar a cadeia intermedi√°ria de roteamento:  
# result = chain_de_roteamento.invoke({"pergunta_user": "Quero falar com um humano"})  
  
# -------------------------------------------------------------------------------- 
  
# Definindo o prompt de chatbot que tira duvidas do usu√°rio:  
  
sys_chatbot_prompt = """ Voc√™ √© um assistente de uma clinica odontol√≥gica e tem como objetivo responder √† perguntas dos clientes. A seguir voc√™ \  
encontra a FAQ do nosso site, use essas informa√ß√µes para realizar o atendimento e tirar d√∫vidas. Caso voc√™ desconhe√ßa alguma \  
informa√ß√£o, n√£o invente. Seja sempre amig√°vel e esteja disposto a ajudar!  
  
**FAQ - Cl√≠nica Odontol√≥gica**  
1. **Quais servi√ßos a cl√≠nica oferece?**    
   Oferecemos tratamentos como limpeza dental, clareamento, ortodontia, implantes, pr√≥teses, tratamento de canal e est√©tica dental.  
2. **A cl√≠nica aceita conv√™nios?**    
   Sim, trabalhamos com os principais conv√™nios odontol√≥gicos. Consulte nossa equipe para verificar se aceitamos o seu.  
3. **Como agendar uma consulta?**    
   Voc√™ pode agendar sua consulta pelo telefone, WhatsApp ou diretamente em nosso site.  
4. **Quanto tempo dura uma consulta?**    
   Depende do procedimento, mas consultas de rotina geralmente duram entre 30 e 60 minutos.  
5. **Voc√™s atendem emerg√™ncias?**    
   Sim, oferecemos atendimento emergencial para dores agudas, traumas ou casos de urg√™ncia.  
6. **√â poss√≠vel parcelar tratamentos?**    
   Sim, oferecemos op√ß√µes de parcelamento. Entre em contato para conhecer os detalhes.  
7. **Crian√ßas podem ser atendidas na cl√≠nica?**    
   Sim, contamos com profissionais especializados em odontopediatria para cuidar dos sorrisos dos pequenos.  
8. **O clareamento dental √© seguro?**    
   Sim, nossos tratamentos de clareamento s√£o realizados com t√©cnicas e produtos seguros, supervisionados por especialistas.  
Se tiver mais d√∫vidas, entre em contato conosco! üòä  
  
D√∫vida do usu√°rio: {pergunta_user}  
"""  
  
prompt_template_chatbot = ChatPromptTemplate.from_messages([("system", sys_chatbot_prompt),])  
  
chain_chatbot = prompt_template_chatbot | model | StrOutputParser()  
  
## Definindo a fun√ß√£o de escolha de roteamento (n√≥ de rota)  
def executa_roteamento(entrada: Rota):  
    if entrada.opcao:  
        print(f"Op√ß√£o classe Pydantic: {entrada.opcao} (Atendimento humano)")  
        return "Atendimento redirecionado para um humano. Favor aguardar alguns minutos que j√° vamos te atender!"  
    else:  
        print(f"Op√ß√£o classe Pydantic: {entrada.opcao} (Atendimento Chatbot)")  
        return RunnableLambda(lambda x: {"pergunta_user": x.pergunta_user}) | chain_chatbot  
  
# -------------------------------------------------------------------------------- 
  
# Crie a cadeia final usando LangChain Expression Language (LCEL)  
chain = chain_de_roteamento | RunnableLambda(executa_roteamento)  
  
# -------------------------------------------------------------------------------- 
  
# Executando nossa chain principal.  
result = chain.invoke({"pergunta_user": "Quais servi√ßos a cl√≠nica oferece?"})  
  
# -------------------------------------------------------------------------------- 
# Imprimindo a saida.  
print("---------------")  
print(result)  
print("---------------")

```